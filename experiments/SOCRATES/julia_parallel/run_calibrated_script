#!/bin/bash

#Submit this script with: sbatch calibrate_script

#SBATCH --time=06:00:00   # walltime (ran out of time (finishing only 4/5) running just one cases for all tau (using 2 hours w/ 10 tasks), for all cases then we need, conservatively, x6 from cases, and x5/4 from incompletion, in time/tasks. We'll chooose x6
#SBATCH --ntasks=72  # number of processor cores (i.e. tasks) (each node I believe on HPC has 64 cpus as per sinfo --Node --long | head -n 10) -- ask for all bc we're running all tau which is 100 per flight (actually 25 caue more than that is hard to get assigned...)
#SBATCH --nodes=1   # number of nodes 
#SBATCH -J "run_calibrated"   # job name
#SBATCH --mem-per-cpu=40G   # memory per CPU core (one job failed runining 6 cases with 6G, trying again w/ 10G...)
#SBATCH --output=slurm_julia_par_%j.out

module purge

run_script=${1?Error: no config file given} # maybe we add later to just default to config.jl in the experiment directory?
# check if script is started via SLURM or bash (https://stackoverflow.com/q/56962129/13331585) | if with SLURM: there variable '$SLURM_JOB_ID' will exist `if [ -n $SLURM_JOB_ID ]` checks if $SLURM_JOB_ID is not an empty string

# if false;  then # need to add quotes here, see https://unix.stackexchange.com/questions/109625/shell-scripting-z-and-n-options-with-if#comment1152278_109631
# if [ -n "$SLURM_JOB_ID" ];  then # need to add quotes here, see https://unix.stackexchange.com/questions/109625/shell-scripting-z-and-n-options-with-if#comment1152278_109631 | seems to crash on HPC even when launched form slurm... idk why... works on clima... just says  command is sh

#     # check the original location through scontrol and $SLURM_JOB_ID
#     thisfile=$(scontrol show job $SLURM_JOBID | awk -F= '/Command=/{print $2}')
# else # otherwise: started with bash. Get the real location.
#     # thisfile=$(realpath $0)
#     thisfile="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)/$(basename "${BASH_SOURCE[0]}")" # the location of this file I think is more versatile than the stack overflow solution (see https://stackoverflow.com/a/9107028/13331585), realpath might be better though idk...
# fi

# scriptDir="$(dirname $0)" # works w/ no slurm but already on interactive noe
# echo $scriptDir
# scriptDir=$(scontrol show job $SLURM_JOBID | awk -F= '/Command=/{print $2}') # works w/ slurm both while and while not on interactive node?
# echo $scriptDir
# scriptDir="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)/$(basename "${BASH_SOURCE[0]}")" # the location of this file I think is more versatile than the stack overflow solution (see https://stackoverflow.com/a/9107028/13331585), realpath might be better though idk...
# echo $scriptDir


# thisfile=/home/jbenjami/Research_Schneider/CliMa/CalibrateEDMF.jl/experiments/SOCRATES_Calibrate_exponential_T_scaling/julia_parallel/run_calibrated_script # can't figure out how to fool proof make this work w/ HPC even on interactive node which has a separate slurm id... this makes it however, not at all transporatable to other machines...
# thisdir=$(dirname $thisfile) # go up to directory from filename...
# experiment_dir=$(dirname $thisdir) # go up to experiment directory...
# log_dir=${experiment_dir}/Output/Logs/

# echo $thisfile

# module load julia/1.9.3 # hdf5/1.10.1 netcdf-c/4.6.1 openmpi/4.0.1 # do we need these then? costa amde it seem like we don't anymore (doesn't work on sampo anyway... and we should have julia loaded already -- might need on clima idk, it's in my bashrc)
# module load julia/1.10.0 mpich/4.0.0 # copied from other experiments


use_expansion=false



if $(grep -q "Red Hat" /etc/redhat-release);  then # copied from /groups/esm/slurm-buildkite/hooks/environment
    os_type="rhel9" # "We are on a RedHat 9 node"
else 
	os_type="centos7" # some of the expansion nodes still seem to be running centos7 for some strange reason? idk...
fi

if [ "$use_expansion" = false ] || [ "$os_type" = "centos7" ]; then # use_expansion is false or os_type is centos7 despite being on expansion node (idk why that is happening lol)
    # module load julia/1.9.3 openmpi/4.1.5 # do we need open mpi?
    module purge
    module load julia/1.10.0 mpich/4.0.0 # copied from other experiments
else
    # cat /etc/os-release # just to be sure
    source /etc/profile.d/modules.sh # # module seems to not be defined when we call this on expansion? idk why... hopefully this does it automatically, see https://climate-machine.slack.com/archives/CKH8UUZHR/p1700178461357699?thread_ts=1700174744.568459&cid=CKH8UUZHR 
    # Try running on expansion cores
    module purge
    export MODULEPATH="/groups/esm/modules:$MODULEPATH" # see https://github.com/CliMA/ClimaModules, provides julia, julia-preferences, mpiwrapper
    export MODULEPATH="/central/software9/Modules/linux-rhel9-x86_64:/central/software9/Modules/linux-rhel9-broadwell:/central/software9/Modules/linux-rhel9-skylake_avx512:/central/software9/Modules/external/modulefiles:$MODULEPATH" # for some reason these aren't getting loaded ... idk why... it's some weird mix of expansion and regular modules  but only the regular centos7 are getting loaded by sbatch idk but not the rhel9 ones..., plus /central/software9/Modules/external/modulefiles for nsight_systems for climacommon
    # module load climacommon # doesnt work bc of nsys
    # module load julia/1.10.1 
    # module load julia-preferences/2024_02_20
    # module load mpiwrapper/2024_02_27 # is this all we need for mpi? the code still complained about libmpi not being found but it still ran? idk...
    module load climacommon
fi

julia --project -e 'using Pkg; Pkg.instantiate(); Pkg.API.precompile()'

julia --project -p 72 $run_script # Not sure how many processors I should go for... maybe at least 1 more than the batch size?
